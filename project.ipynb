{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0100ad60",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigError",
     "evalue": "unable to infer type for attribute \"chroma_server_nofile\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConfigError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Embeddings and Vector Store\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# LLM\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgroq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Groq\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIRAG\\.venv\\Lib\\site-packages\\chromadb\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client \u001b[38;5;28;01mas\u001b[39;00m ClientCreator\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     AdminClient \u001b[38;5;28;01mas\u001b[39;00m AdminClientCreator,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncClient \u001b[38;5;28;01mas\u001b[39;00m AsyncClientCreator\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIRAG\\.venv\\Lib\\site-packages\\chromadb\\api\\__init__.py:47\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moverrides\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcollection_configuration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     44\u001b[39m     CreateCollectionConfiguration,\n\u001b[32m     45\u001b[39m     UpdateCollectionConfiguration,\n\u001b[32m     46\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_DATABASE, DEFAULT_TENANT\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     49\u001b[39m     CollectionMetadata,\n\u001b[32m     50\u001b[39m     Documents,\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m     DefaultEmbeddingFunction,\n\u001b[32m     69\u001b[39m )\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UserIdentity\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIRAG\\.venv\\Lib\\site-packages\\chromadb\\config.py:120\u001b[39m\n\u001b[32m    116\u001b[39m     NODE = \u001b[33m\"\u001b[39m\u001b[33mnode\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m     ID = \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mSettings\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseSettings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ==============\u001b[39;49;00m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Generic config\u001b[39;49;00m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ==============\u001b[39;49;00m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Can be \"chromadb.api.segment.SegmentAPI\" or \"chromadb.api.fastapi.FastAPI\" or \"chromadb.api.rust.RustBindingsAPI\"\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIRAG\\.venv\\Lib\\site-packages\\pydantic\\v1\\main.py:221\u001b[39m, in \u001b[36mModelMetaclass.__new__\u001b[39m\u001b[34m(mcs, name, bases, namespace, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_valid_field(var_name) \u001b[38;5;129;01mand\u001b[39;00m var_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m annotations \u001b[38;5;129;01mand\u001b[39;00m can_be_changed:\n\u001b[32m    220\u001b[39m     validate_field_name(bases, var_name)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     inferred = \u001b[43mModelField\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUndefined\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_validators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(inferred.type_, fields[var_name].type_):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIRAG\\.venv\\Lib\\site-packages\\pydantic\\v1\\fields.py:504\u001b[39m, in \u001b[36mModelField.infer\u001b[39m\u001b[34m(cls, name, value, annotation, class_validators, config)\u001b[39m\n\u001b[32m    501\u001b[39m     required = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    502\u001b[39m annotation = get_annotation_from_field_info(annotation, field_info, name, config.validate_assignment)\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43malias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequired\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequired\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIRAG\\.venv\\Lib\\site-packages\\pydantic\\v1\\fields.py:434\u001b[39m, in \u001b[36mModelField.__init__\u001b[39m\u001b[34m(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28mself\u001b[39m.shape: \u001b[38;5;28mint\u001b[39m = SHAPE_SINGLETON\n\u001b[32m    433\u001b[39m \u001b[38;5;28mself\u001b[39m.model_config.prepare_field(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIRAG\\.venv\\Lib\\site-packages\\pydantic\\v1\\fields.py:544\u001b[39m, in \u001b[36mModelField.prepare\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    538\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[33;03m    Prepare the field but inspecting self.default, self.type_ etc.\u001b[39;00m\n\u001b[32m    540\u001b[39m \n\u001b[32m    541\u001b[39m \u001b[33;03m    Note: this method is **not** idempotent (because _type_analysis is not idempotent),\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03m    e.g. calling it it multiple times may modify the field and configure it incorrectly.\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_default_and_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m ForwardRef \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m DeferredType:\n\u001b[32m    546\u001b[39m         \u001b[38;5;66;03m# self.type_ is currently a ForwardRef and there's nothing we can do now,\u001b[39;00m\n\u001b[32m    547\u001b[39m         \u001b[38;5;66;03m# user will need to call model.update_forward_refs()\u001b[39;00m\n\u001b[32m    548\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIRAG\\.venv\\Lib\\site-packages\\pydantic\\v1\\fields.py:576\u001b[39m, in \u001b[36mModelField._set_default_and_type\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    573\u001b[39m     \u001b[38;5;28mself\u001b[39m.annotation = \u001b[38;5;28mself\u001b[39m.type_\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_ \u001b[38;5;129;01mis\u001b[39;00m Undefined:\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m errors_.ConfigError(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33munable to infer type for attribute \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.required \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28mself\u001b[39m.allow_none = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConfigError\u001b[39m: unable to infer type for attribute \"chroma_server_nofile\""
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embeddings and Vector Store\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# LLM\n",
    "from groq import Groq\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… All imports successful\")\n",
    "\n",
    "\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  âœ… Loaded {len(documents)} pages\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Total documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# ============================================================================\n",
    "# 2. TEXT SPLITTING\n",
    "# ============================================================================\n",
    "\n",
    "def split_documents(documents, chunk_size=800, chunk_overlap=100):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # âœ… Fixed typo\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"âœ… Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "# ============================================================================\n",
    "# 3. EMBEDDING MANAGER\n",
    "# ============================================================================\n",
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: Hugging Face model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"âœ… Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "        Returns:\n",
    "            Numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f'Generating embeddings for {len(texts)} texts...')\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f'âœ… Generated embeddings with shape: {embeddings.shape}')\n",
    "        return embeddings\n",
    "\n",
    "# ============================================================================\n",
    "# 4. VECTOR STORE\n",
    "# ============================================================================\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = 'pdf_documents', \n",
    "                 persist_directory: str = \"data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    \n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f'âœ… Vector store initialized. Collection: {self.collection_name}')\n",
    "            print(f'   Existing documents in collection: {self.collection.count()}')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error initializing vector store: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"âœ… Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"   Total documents in collection: {self.collection.count()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "# ============================================================================\n",
    "# 5. RAG RETRIEVER\n",
    "# ============================================================================\n",
    "\n",
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ” Retrieving documents for query: '{query[:50]}...'\")\n",
    "        print(f\"   Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"âœ… Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"âš ï¸ No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "# ============================================================================\n",
    "# 6. GROQ LLM CLIENT\n",
    "# ============================================================================\n",
    "\n",
    "class GroqClient:\n",
    "    \"\"\"Client to interact with Groq LLM API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the Groq client\n",
    "        \n",
    "        Args:\n",
    "            api_key: Groq API key (if None, reads from GROQ_API_KEY env variable)\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"Groq API key not found. Set GROQ_API_KEY environment variable or pass api_key parameter.\"\n",
    "            )\n",
    "        \n",
    "        self.client = self._initialize_client()\n",
    "    \n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize Groq client with API key\"\"\"\n",
    "        try:\n",
    "            print(\"Initializing Groq client...\")\n",
    "            client = Groq(api_key=self.api_key)\n",
    "            print(\"âœ… Groq client initialized successfully\")\n",
    "            return client\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error initializing Groq client: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_response(\n",
    "        self,\n",
    "        query: str,\n",
    "        contexts: List[Dict[str, Any]],\n",
    "        model: str = \"llama-3.3-70b-versatile\",\n",
    "        temperature: float = 0.3,\n",
    "        max_tokens: int = 1000\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved contexts\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            contexts: List of retrieved documents from RAGRetriever\n",
    "            model: Groq model to use\n",
    "            temperature: Response creativity (0.0-1.0)\n",
    "            max_tokens: Maximum response length\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        if not contexts:\n",
    "            return {\n",
    "                \"answer\": \"I couldn't find any relevant information in the documents to answer your question.\",\n",
    "                \"sources\": [],\n",
    "                \"model\": model,\n",
    "                \"num_contexts_used\": 0\n",
    "            }\n",
    "        \n",
    "        # Format contexts with numbering for citations\n",
    "        formatted_contexts = []\n",
    "        for i, ctx in enumerate(contexts, 1):\n",
    "            source = ctx['metadata'].get('source_file', 'Unknown')\n",
    "            page = ctx['metadata'].get('page', 'N/A')\n",
    "            similarity = ctx.get('similarity_score', 0.0)\n",
    "            \n",
    "            formatted_contexts.append(\n",
    "                f\"[{i}] (Source: {source}, Page: {page}, Relevance: {similarity:.1%})\\n{ctx['content']}\"\n",
    "            )\n",
    "        \n",
    "        context_text = \"\\n\\n\".join(formatted_contexts)\n",
    "        \n",
    "        # Create structured prompt\n",
    "        prompt = f\"\"\"You are a helpful AI assistant. Answer the user's question based ONLY on the provided context from PDF documents.\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "1. Only use information from the context below\n",
    "2. Cite sources using [1], [2], [3] format after relevant statements\n",
    "3. If the context doesn't contain enough information, clearly state that\n",
    "4. Be concise but comprehensive\n",
    "5. If you're uncertain, express appropriate confidence levels\n",
    "\n",
    "CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER (with citations):\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content\n",
    "            \n",
    "            # Extract source information\n",
    "            sources = [\n",
    "                {\n",
    "                    \"id\": i + 1,\n",
    "                    \"file\": ctx['metadata'].get('source_file', 'Unknown'),\n",
    "                    \"page\": ctx['metadata'].get('page', 'N/A'),\n",
    "                    \"similarity\": ctx.get('similarity_score', 0.0)\n",
    "                }\n",
    "                for i, ctx in enumerate(contexts)\n",
    "            ]\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": sources,\n",
    "                \"model\": model,\n",
    "                \"num_contexts_used\": len(contexts)\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error generating response: {e}\")\n",
    "            return {\n",
    "                \"answer\": f\"Error generating response: {str(e)}\",\n",
    "                \"sources\": [],\n",
    "                \"model\": model,\n",
    "                \"num_contexts_used\": 0\n",
    "            }\n",
    "\n",
    "# ============================================================================\n",
    "# 7. RAG SYSTEM (ORCHESTRATOR)\n",
    "# ============================================================================\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"Complete RAG system that orchestrates all components\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store, embedding_manager, llm_client, retriever=None):\n",
    "        \"\"\"\n",
    "        Initialize RAG system\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store instance\n",
    "            embedding_manager: Embedding manager instance\n",
    "            llm_client: LLM client instance\n",
    "            retriever: Optional pre-initialized retriever\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.llm_client = llm_client\n",
    "        self.retriever = retriever if retriever is not None else RAGRetriever(\n",
    "            self.vector_store, self.embedding_manager\n",
    "        )\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        print(\"âœ… RAG System initialized successfully\")\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 5, score_threshold: float = 0.5, **llm_kwargs):\n",
    "        \"\"\"\n",
    "        Query the RAG system\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            top_k: Number of documents to retrieve\n",
    "            score_threshold: Minimum similarity threshold\n",
    "            **llm_kwargs: Additional arguments for LLM (model, temperature, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            Response dictionary with answer, sources, and metadata\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'â”€'*70}\")\n",
    "        print(f\"ğŸ“ Query: {question}\")\n",
    "        print(f\"{'â”€'*70}\\n\")\n",
    "        \n",
    "        # Retrieve contexts\n",
    "        contexts = self.retriever.retrieve(\n",
    "            question,\n",
    "            top_k=top_k,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        if not contexts:\n",
    "            print(\"âš ï¸ No relevant contexts found\")\n",
    "            response = {\n",
    "                'answer': 'I could not find relevant information to answer this question.',\n",
    "                'sources': [],\n",
    "                'num_contexts_used': 0,\n",
    "                'question': question\n",
    "            }\n",
    "        else:\n",
    "            # Generate response\n",
    "            response = self.llm_client.generate_response(\n",
    "                query=question,\n",
    "                contexts=contexts,\n",
    "                **llm_kwargs\n",
    "            )\n",
    "            response['question'] = question\n",
    "            response['retrieved_contexts'] = contexts\n",
    "        \n",
    "        # Track conversation\n",
    "        self.conversation_history.append({\n",
    "            'question': question,\n",
    "            'answer': response.get('answer'),\n",
    "            'sources': response.get('sources', []),\n",
    "            'num_contexts': response.get('num_contexts_used', 0)\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def display_response(self, response: dict):\n",
    "        \"\"\"Display response in a formatted way\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âœ… ANSWER\")\n",
    "        print(\"=\"*70)\n",
    "        print(response.get('answer', 'No answer returned'))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"ğŸ“„ SOURCES ({response.get('num_contexts_used', 0)} documents)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        sources = response.get('sources', [])\n",
    "        if not sources:\n",
    "            print(\"  No sources available\")\n",
    "        else:\n",
    "            for src in sources:\n",
    "                if isinstance(src, dict):\n",
    "                    print(f\"  [{src.get('id', '?')}] {src.get('file', 'Unknown')} \"\n",
    "                          f\"(Page {src.get('page', 'N/A')}) - \"\n",
    "                          f\"Relevance: {src.get('similarity', 0.0):.1%}\")\n",
    "                else:\n",
    "                    print(f\"  - {src}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¤– Model: {response.get('model', 'N/A')} | \"\n",
    "              f\"Contexts: {response.get('num_contexts_used', 0)}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get session statistics\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return {\n",
    "                \"total_queries\": 0,\n",
    "                \"total_contexts_retrieved\": 0,\n",
    "                \"avg_contexts_per_query\": 0.0\n",
    "            }\n",
    "        \n",
    "        total_contexts = sum(c['num_contexts'] for c in self.conversation_history)\n",
    "        return {\n",
    "            \"total_queries\": len(self.conversation_history),\n",
    "            \"total_contexts_retrieved\": total_contexts,\n",
    "            \"avg_contexts_per_query\": total_contexts / len(self.conversation_history)\n",
    "        }\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"âœ… Conversation history cleared\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ INITIALIZING RAG SYSTEM\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 1: Load PDFs\n",
    "print(\"Step 1: Loading PDFs...\")\n",
    "all_pdf_documents = process_all_pdfs(\"data/pdf\")\n",
    "\n",
    "# Step 2: Split into chunks\n",
    "print(\"\\nStep 2: Splitting documents...\")\n",
    "chunks = split_documents(all_pdf_documents, chunk_size=800, chunk_overlap=100)\n",
    "\n",
    "# Step 3: Initialize Embedding Manager\n",
    "print(\"\\nStep 3: Initializing Embedding Manager...\")\n",
    "embedding_manager = EmbeddingManager()\n",
    "\n",
    "# Step 4: Generate embeddings\n",
    "print(\"\\nStep 4: Generating embeddings...\")\n",
    "texts = [chunk.page_content for chunk in chunks]  # âœ… Fixed: Extract text from chunks\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Step 5: Initialize Vector Store\n",
    "print(\"\\nStep 5: Initializing Vector Store...\")\n",
    "vectorstore = VectorStore()\n",
    "\n",
    "# Step 6: Add documents to vector store\n",
    "print(\"\\nStep 6: Adding documents to vector store...\")\n",
    "vectorstore.add_documents(chunks, embeddings)\n",
    "\n",
    "# Step 7: Initialize Groq Client\n",
    "print(\"\\nStep 7: Initializing Groq LLM Client...\")\n",
    "groq_client = GroqClient()  # Reads GROQ_API_KEY from environment\n",
    "\n",
    "# Step 8: Create RAG System\n",
    "print(\"\\nStep 8: Creating RAG System...\")\n",
    "rag_system = RAGSystem(\n",
    "    vector_store=vectorstore,\n",
    "    embedding_manager=embedding_manager,\n",
    "    llm_client=groq_client\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… SETUP COMPLETE - Ready to Query!\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nâ±ï¸ Total setup time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de01c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ¤– RUNNING TEST QUERIES\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "QUERY 1/3\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: What are the main topics of artificial intelligence covered in the documents?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'What are the main topics of artificial intelligenc...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1858af66b51c4b548fd9bd6196430ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "QUERY 2/3\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: Explain the key findings from the research papers about neural networks.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'Explain the key findings from the research papers ...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882a1775c25f41ba9e4b25e043724033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "QUERY 3/3\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: What methodologies are discussed regarding agentic ai?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'What methodologies are discussed regarding agentic...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41f1f7d059f438b980b3e3cad210a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 1 documents (after filtering)\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "The methodologies discussed regarding Agentic AI include Architectural Approaches, Learning Paradigms, Training Techniques, and Tools [1]. These methodologies are outlined in Figure 3 of the provided source, which gives an overview of Agentic AI Development Methodologies [1].\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (1 documents)\n",
      "======================================================================\n",
      "  [1] Agentic_AI_Autonomous_Intelligence_for_Complex_GoalsA_Comprehensive_Survey.pdf (Page 8) - Relevance: 52.2%\n",
      "\n",
      "ğŸ¤– Model: llama-3.3-70b-versatile | Contexts: 1\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š SESSION STATISTICS\n",
      "======================================================================\n",
      "Total Queries: 10\n",
      "Total Contexts Retrieved: 1\n",
      "Avg Contexts per Query: 0.1\n",
      "======================================================================\n",
      "\n",
      "âœ… RAG System ready for interactive queries!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 9. EXAMPLE QUERIES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¤– RUNNING TEST QUERIES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What are the main topics of artificial intelligence covered in the documents?\",\n",
    "    \"Explain the key findings from the research papers about neural networks.\",\n",
    "    \"What methodologies are discussed regarding agentic ai?\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'â”'*70}\")\n",
    "    print(f\"QUERY {i}/{len(test_queries)}\")\n",
    "    print(f\"{'â”'*70}\")\n",
    "    \n",
    "    response = rag_system.query(\n",
    "        question=query,\n",
    "        top_k=3,\n",
    "        score_threshold=0.5,\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    rag_system.display_response(response)\n",
    "\n",
    "# ============================================================================\n",
    "# 10. SESSION STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š SESSION STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "stats = rag_system.get_stats()\n",
    "print(f\"Total Queries: {stats['total_queries']}\")\n",
    "print(f\"Total Contexts Retrieved: {stats['total_contexts_retrieved']}\")\n",
    "print(f\"Avg Contexts per Query: {stats['avg_contexts_per_query']:.1f}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"âœ… RAG System ready for interactive queries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944063cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¬ Interactive Mode - Ask questions about your documents!\n",
      "   (Type 'exit' to stop)\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: what is artificial intelligence?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'what is artificial intelligence?...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd4654d86fc48f59e4e00a9f14f987a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: how to implement RAG system\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'how to implement RAG system...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d84e6109a8247fc93088b9c0ac19f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: RAG\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'RAG...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b08c02b03594b8587c905d939faf53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: hello\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'hello...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0adee5a5ac459db6f73aeb6538fdfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: What is ML\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'What is ML...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af839b0a5d834fb5b79a8ad3363dd981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "ğŸ‘‹ Goodbye!\n",
      "Testing conversation memory...\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: \n",
      "\n",
      "Previous conversation:\n",
      "Q: hello\n",
      "A: I could not find relevant information to answer this question....\n",
      "Q: What is ML\n",
      "A: I could not find relevant information to answer this question....\n",
      "\n",
      "\n",
      "Current question: What is RAG?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: '\n",
      "\n",
      "Previous conversation:\n",
      "Q: hello\n",
      "A: I could not f...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815f349ea062492aa0c18c8d323f80a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: \n",
      "\n",
      "Previous conversation:\n",
      "Q: What is ML\n",
      "A: I could not find relevant information to answer this question....\n",
      "Q: \n",
      "\n",
      "Previous conversation:\n",
      "Q: hello\n",
      "A: I could not find relevant information to answer this question....\n",
      "Q: What is ML\n",
      "A: I could not find relevant information to answer this question....\n",
      "\n",
      "\n",
      "Current question: What is RAG?\n",
      "A: I could not find relevant information to answer this question....\n",
      "\n",
      "\n",
      "Current question: What are its benefits?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: '\n",
      "\n",
      "Previous conversation:\n",
      "Q: What is ML\n",
      "A: I could ...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2263b41f37034763830cf2e86cf54d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INTERACTIVE QUERY MODE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ’¬ Interactive Mode - Ask questions about your documents!\")\n",
    "print(\"   (Type 'exit' to stop)\\n\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"Your question: \").strip()\n",
    "    \n",
    "    if user_query.lower() in ['exit', 'quit', 'q', '']:\n",
    "        print(\"\\nğŸ‘‹ Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    response = rag_system.query(\n",
    "        question=user_query,\n",
    "        top_k=3,\n",
    "        score_threshold=0.5,\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.3,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    \n",
    "    rag_system.display_response(response)\n",
    "\n",
    "    # Add this cell after your interactive mode\n",
    "\n",
    "def query_with_memory(rag_system, question: str, top_k: int = 3):\n",
    "    \"\"\"Query with awareness of previous conversation\"\"\"\n",
    "    # Get last 2 exchanges\n",
    "    history = rag_system.conversation_history[-2:] if rag_system.conversation_history else []\n",
    "    \n",
    "    history_context = \"\"\n",
    "    if history:\n",
    "        history_context = \"\\n\\nPrevious conversation:\\n\"\n",
    "        for conv in history:\n",
    "            history_context += f\"Q: {conv['question']}\\nA: {conv['answer'][:100]}...\\n\"\n",
    "    \n",
    "    enhanced_question = f\"{history_context}\\n\\nCurrent question: {question}\"\n",
    "    \n",
    "    response = rag_system.query(\n",
    "        question=enhanced_question,\n",
    "        top_k=top_k,\n",
    "        score_threshold=0.5\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Test multi-turn conversation\n",
    "print(\"Testing conversation memory...\\n\")\n",
    "q1 = query_with_memory(rag_system, \"What is RAG?\")\n",
    "rag_system.display_response(q1)\n",
    "\n",
    "q2 = query_with_memory(rag_system, \"What are its benefits?\")\n",
    "rag_system.display_response(q2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f718e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved to rag_saves/\n",
      "   Config: my_rag_session_config.json\n",
      "   Conversations: my_rag_session.db\n"
     ]
    }
   ],
   "source": [
    "# %% Install (run once)\n",
    "# !pip install pandas\n",
    "\n",
    "# %% SAVE & LOAD SYSTEM - Recommended Approach\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class RAGSystemSaver:\n",
    "    \"\"\"Simple, secure way to save/load your RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir=\"rag_saves\"):\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    def save_all(self, rag_system, name=\"default\"):\n",
    "        \"\"\"Save everything in one call\"\"\"\n",
    "        \n",
    "        # 1. Save configuration as JSON\n",
    "        config = {\n",
    "            'name': name,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'vector_store': {\n",
    "                'path': rag_system.vector_store.persist_directory,\n",
    "                'collection_name': rag_system.vector_store.collection_name,\n",
    "                'total_documents': rag_system.vector_store.collection.count()\n",
    "            },\n",
    "            'embedding_model': rag_system.embedding_manager.model_name,\n",
    "            'total_conversations': len(rag_system.conversation_history)\n",
    "        }\n",
    "        \n",
    "        with open(f\"{self.save_dir}/{name}_config.json\", 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        # 2. Save conversations to SQLite\n",
    "        db_path = f\"{self.save_dir}/{name}.db\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS conversations (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                question TEXT,\n",
    "                answer TEXT,\n",
    "                num_contexts INTEGER,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Insert conversations\n",
    "        for conv in rag_system.conversation_history:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO conversations (question, answer, num_contexts)\n",
    "                VALUES (?, ?, ?)\n",
    "            ''', (conv['question'], conv['answer'], conv['num_contexts']))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"âœ… Saved to {self.save_dir}/\")\n",
    "        print(f\"   Config: {name}_config.json\")\n",
    "        print(f\"   Conversations: {name}.db\")\n",
    "\n",
    "# Usage:\n",
    "saver = RAGSystemSaver()\n",
    "saver.save_all(rag_system, name=\"my_rag_session\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57f984dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing query expansion...\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'Compare different RAG architectures...'\n",
      "   Top K: 2, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1e2ff159324e51acc6c538dd021eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "ğŸ” Retrieving documents for query: 'What are the key differences between various RAG a...'\n",
      "   Top K: 2, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8740729240894d32886d856598a5ffd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "ğŸ” Retrieving documents for query: 'How do distinct RAG architectures stack up against...'\n",
      "   Top K: 2, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60280f56e2494b64965529a9d676c66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "Retrieved 0 unique contexts using query expansion\n"
     ]
    }
   ],
   "source": [
    "def expand_query(rag_system, question: str):\n",
    "    \"\"\"Generate alternative phrasings to improve retrieval\"\"\"\n",
    "    prompt = f\"\"\"Generate 2 alternative phrasings of this question:\n",
    "\"{question}\"\n",
    "\n",
    "Return ONLY the 2 questions, one per line. No numbering.\"\"\"\n",
    "    \n",
    "    response = rag_system.llm_client.client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    alternatives = response.choices[0].message.content.strip().split('\\n')\n",
    "    return [question] + alternatives[:2]\n",
    "\n",
    "def multi_query_retrieve(rag_system, question: str, top_k: int = 3):\n",
    "    \"\"\"Retrieve using multiple query variations\"\"\"\n",
    "    queries = expand_query(rag_system, question)\n",
    "    \n",
    "    all_contexts = []\n",
    "    seen_docs = set()\n",
    "    \n",
    "    for q in queries:\n",
    "        contexts = rag_system.retriever.retrieve(q, top_k=2, score_threshold=0.5)\n",
    "        for ctx in contexts:\n",
    "            doc_id = f\"{ctx['metadata'].get('source_file')}_{ctx['metadata'].get('page')}\"\n",
    "            if doc_id not in seen_docs:\n",
    "                all_contexts.append(ctx)\n",
    "                seen_docs.add(doc_id)\n",
    "    \n",
    "    return all_contexts[:top_k]\n",
    "\n",
    "# Test multi-query retrieval\n",
    "print(\"Testing query expansion...\\n\")\n",
    "expanded_question = \"Compare different RAG architectures\"\n",
    "better_contexts = multi_query_retrieve(rag_system, expanded_question)\n",
    "print(f\"Retrieved {len(better_contexts)} unique contexts using query expansion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afd49b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: What are the main findings?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'What are the main findings?...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426e22de0da64210a6bfe794dd5e1103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query_with_confidence_check(rag_system, question: str, top_k: int = 3):\n",
    "    \"\"\"Query with confidence threshold warnings\"\"\"\n",
    "    response = rag_system.query(question, top_k=top_k, score_threshold=0.5)\n",
    "    \n",
    "    if response.get('sources'):\n",
    "        avg_similarity = np.mean([src['similarity'] for src in response['sources']])\n",
    "        \n",
    "        if avg_similarity < 0.6:\n",
    "            print(f\"âš ï¸ WARNING: Low confidence score ({avg_similarity:.1%})\")\n",
    "            print(\"   Results may be unreliable. Try a different question.\\n\")\n",
    "        elif avg_similarity < 0.75:\n",
    "            print(f\"â„¹ï¸ MODERATE confidence ({avg_similarity:.1%})\")\n",
    "        else:\n",
    "            print(f\"âœ… HIGH confidence ({avg_similarity:.1%})\\n\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test\n",
    "response = query_with_confidence_check(rag_system, \"What are the main findings?\")\n",
    "rag_system.display_response(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
