{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0100ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ INITIALIZING RAG SYSTEM\n",
      "======================================================================\n",
      "\n",
      "Step 1: Loading PDFs...\n",
      "Found 6 PDF files to process\n",
      "\n",
      "Processing: Agentic_AI_Autonomous_Intelligence_for_Complex_GoalsA_Comprehensive_Survey.pdf\n",
      "  âœ… Loaded 25 pages\n",
      "\n",
      "Processing: AI.pdf\n",
      "  âœ… Loaded 71 pages\n",
      "\n",
      "Processing: LLM.pdf\n",
      "  âœ… Loaded 29 pages\n",
      "\n",
      "Processing: ML.pdf\n",
      "  âœ… Loaded 12 pages\n",
      "\n",
      "Processing: National-Strategy-for-Artificial-Intelligence.pdf\n",
      "  âœ… Loaded 114 pages\n",
      "\n",
      "Processing: RAG.pdf\n",
      "  âœ… Loaded 21 pages\n",
      "\n",
      "âœ… Total documents loaded: 272\n",
      "\n",
      "Step 2: Splitting documents...\n",
      "âœ… Split 272 documents into 1321 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Received 1 January 2025, accepted 19 January 2025, date of publication 22 January 2025, date of current version 29 January 2025.\n",
      "Digital Object Identifier 10.1 109/ACCESS.2025.3532853\n",
      "Agentic AI: Auto...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.24; modified using iTextÂ® Core 7.2.4 (AGPL version) Â©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-28T21:29:41+05:30', 'moddate': '2025-01-29T10:28:02-05:00', 'ieee article id': '10849561', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3532853', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Agentic AI: Autonomous Intelligence for Complex Goals&#x2014;A Comprehensive Survey', 'source': 'data\\\\pdf\\\\Agentic_AI_Autonomous_Intelligence_for_Complex_GoalsA_Comprehensive_Survey.pdf', 'total_pages': 25, 'page': 0, 'page_label': '18912', 'source_file': 'Agentic_AI_Autonomous_Intelligence_for_Complex_GoalsA_Comprehensive_Survey.pdf', 'file_type': 'pdf'}\n",
      "\n",
      "Step 3: Initializing Embedding Manager...\n",
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "âœ… Model loaded successfully. Embedding dimension: 384\n",
      "\n",
      "Step 4: Generating embeddings...\n",
      "Generating embeddings for 1321 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b7a2c5ddca4bd4a813c81761c4d609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1321, 384)\n",
      "âœ… FAISS vector store initialized\n",
      "\n",
      "Step 6: Adding documents to vector store...\n",
      "âœ… Added 1321 documents to FAISS index\n",
      "   Total vectors: 1321\n",
      "\n",
      "Step 7: Initializing Groq LLM Client...\n",
      "Initializing Groq client...\n",
      "âœ… Groq client initialized successfully\n",
      "\n",
      "Step 8: Creating RAG System...\n",
      "âœ… RAG System initialized successfully\n",
      "\n",
      "======================================================================\n",
      "âœ… SETUP COMPLETE - Ready to Query!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â±ï¸ Total setup time: 62.44 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embeddings and Vector Store\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# LLM\n",
    "from groq import Groq\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… All imports successful\")\n",
    "\n",
    "\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  âœ… Loaded {len(documents)} pages\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Total documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# ============================================================================\n",
    "# 2. TEXT SPLITTING\n",
    "# ============================================================================\n",
    "\n",
    "def split_documents(documents, chunk_size=800, chunk_overlap=100):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # âœ… Fixed typo\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"âœ… Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "# ============================================================================\n",
    "# 3. EMBEDDING MANAGER\n",
    "# ============================================================================\n",
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: Hugging Face model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"âœ… Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "        Returns:\n",
    "            Numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f'Generating embeddings for {len(texts)} texts...')\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f'âœ… Generated embeddings with shape: {embeddings.shape}')\n",
    "        return embeddings\n",
    "\n",
    "# ============================================================================\n",
    "# 4. VECTOR STORE\n",
    "# ============================================================================\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings using FAISS\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int):\n",
    "        \"\"\"\n",
    "        Initialize FAISS vector store\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimension of embeddings\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index = faiss.IndexFlatIP(embedding_dim)  # cosine similarity\n",
    "        self.documents = []\n",
    "        self.metadatas = []\n",
    "        print(\"âœ… FAISS vector store initialized\")\n",
    "    \n",
    "    def add_documents(self, documents, embeddings):\n",
    "        \"\"\"\n",
    "        Add documents and embeddings to FAISS index\n",
    "        \"\"\"\n",
    "        embeddings = embeddings.astype(\"float32\")\n",
    "        faiss.normalize_L2(embeddings)  # required for cosine similarity\n",
    "        \n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        for doc in documents:\n",
    "            self.documents.append(doc.page_content)\n",
    "            self.metadatas.append(doc.metadata)\n",
    "        \n",
    "        print(f\"âœ… Added {len(documents)} documents to FAISS index\")\n",
    "        print(f\"   Total vectors: {self.index.ntotal}\")\n",
    "    \n",
    "    def search(self, query_embedding, top_k=5):\n",
    "        \"\"\"\n",
    "        Search FAISS index\n",
    "        \"\"\"\n",
    "        query_embedding = np.array(query_embedding, dtype=\"float32\").reshape(1, -1)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        results = []\n",
    "        for rank, idx in enumerate(indices[0]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            \n",
    "            results.append({\n",
    "                \"content\": self.documents[idx],\n",
    "                \"metadata\": self.metadatas[idx],\n",
    "                \"similarity_score\": float(distances[0][rank]),\n",
    "                \"rank\": rank + 1\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "# ============================================================================\n",
    "# 5. RAG RETRIEVER\n",
    "# ============================================================================\n",
    "\n",
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0):\n",
    "        print(f\"ğŸ” Retrieving documents for query: '{query[:50]}...'\")\n",
    "        print(f\"   Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # FAISS search\n",
    "        results = self.vector_store.search(\n",
    "            query_embedding=query_embedding,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        \n",
    "        # Apply score threshold\n",
    "        filtered_results = [\n",
    "            r for r in results\n",
    "            if r[\"similarity_score\"] >= score_threshold\n",
    "        ]\n",
    "        \n",
    "        print(f\"âœ… Retrieved {len(filtered_results)} documents\")\n",
    "        return filtered_results\n",
    "\n",
    "# ============================================================================\n",
    "# 6. GROQ LLM CLIENT\n",
    "# ============================================================================\n",
    "\n",
    "class GroqClient:\n",
    "    \"\"\"Client to interact with Groq LLM API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the Groq client\n",
    "        \n",
    "        Args:\n",
    "            api_key: Groq API key (if None, reads from GROQ_API_KEY env variable)\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"Groq API key not found. Set GROQ_API_KEY environment variable or pass api_key parameter.\"\n",
    "            )\n",
    "        \n",
    "        self.client = self._initialize_client()\n",
    "    \n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize Groq client with API key\"\"\"\n",
    "        try:\n",
    "            print(\"Initializing Groq client...\")\n",
    "            client = Groq(api_key=self.api_key)\n",
    "            print(\"âœ… Groq client initialized successfully\")\n",
    "            return client\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error initializing Groq client: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_response(\n",
    "        self,\n",
    "        query: str,\n",
    "        contexts: List[Dict[str, Any]],\n",
    "        model: str = \"llama-3.3-70b-versatile\",\n",
    "        temperature: float = 0.3,\n",
    "        max_tokens: int = 1000\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved contexts\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            contexts: List of retrieved documents from RAGRetriever\n",
    "            model: Groq model to use\n",
    "            temperature: Response creativity (0.0-1.0)\n",
    "            max_tokens: Maximum response length\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        if not contexts:\n",
    "            return {\n",
    "                \"answer\": \"I couldn't find any relevant information in the documents to answer your question.\",\n",
    "                \"sources\": [],\n",
    "                \"model\": model,\n",
    "                \"num_contexts_used\": 0\n",
    "            }\n",
    "        \n",
    "        # Format contexts with numbering for citations\n",
    "        formatted_contexts = []\n",
    "        for i, ctx in enumerate(contexts, 1):\n",
    "            source = ctx['metadata'].get('source_file', 'Unknown')\n",
    "            page = ctx['metadata'].get('page', 'N/A')\n",
    "            similarity = ctx.get('similarity_score', 0.0)\n",
    "            \n",
    "            formatted_contexts.append(\n",
    "                f\"[{i}] (Source: {source}, Page: {page}, Relevance: {similarity:.1%})\\n{ctx['content']}\"\n",
    "            )\n",
    "        \n",
    "        context_text = \"\\n\\n\".join(formatted_contexts)\n",
    "        \n",
    "        # Create structured prompt\n",
    "        prompt = f\"\"\"You are a helpful AI assistant. Answer the user's question based ONLY on the provided context from PDF documents.\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "1. Only use information from the context below\n",
    "2. Cite sources using [1], [2], [3] format after relevant statements\n",
    "3. If the context doesn't contain enough information, clearly state that\n",
    "4. Be concise but comprehensive\n",
    "5. If you're uncertain, express appropriate confidence levels\n",
    "\n",
    "CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER (with citations):\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content\n",
    "            \n",
    "            # Extract source information\n",
    "            sources = [\n",
    "                {\n",
    "                    \"id\": i + 1,\n",
    "                    \"file\": ctx['metadata'].get('source_file', 'Unknown'),\n",
    "                    \"page\": ctx['metadata'].get('page', 'N/A'),\n",
    "                    \"similarity\": ctx.get('similarity_score', 0.0)\n",
    "                }\n",
    "                for i, ctx in enumerate(contexts)\n",
    "            ]\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": sources,\n",
    "                \"model\": model,\n",
    "                \"num_contexts_used\": len(contexts)\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error generating response: {e}\")\n",
    "            return {\n",
    "                \"answer\": f\"Error generating response: {str(e)}\",\n",
    "                \"sources\": [],\n",
    "                \"model\": model,\n",
    "                \"num_contexts_used\": 0\n",
    "            }\n",
    "\n",
    "# ============================================================================\n",
    "# 7. RAG SYSTEM (ORCHESTRATOR)\n",
    "# ============================================================================\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"Complete RAG system that orchestrates all components\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store, embedding_manager, llm_client, retriever=None):\n",
    "        \"\"\"\n",
    "        Initialize RAG system\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store instance\n",
    "            embedding_manager: Embedding manager instance\n",
    "            llm_client: LLM client instance\n",
    "            retriever: Optional pre-initialized retriever\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.llm_client = llm_client\n",
    "        self.retriever = retriever if retriever is not None else RAGRetriever(\n",
    "            self.vector_store, self.embedding_manager\n",
    "        )\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        print(\"âœ… RAG System initialized successfully\")\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 5, score_threshold: float = 0.5, **llm_kwargs):\n",
    "        \"\"\"\n",
    "        Query the RAG system\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            top_k: Number of documents to retrieve\n",
    "            score_threshold: Minimum similarity threshold\n",
    "            **llm_kwargs: Additional arguments for LLM (model, temperature, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            Response dictionary with answer, sources, and metadata\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'â”€'*70}\")\n",
    "        print(f\"ğŸ“ Query: {question}\")\n",
    "        print(f\"{'â”€'*70}\\n\")\n",
    "        \n",
    "        # Retrieve contexts\n",
    "        contexts = self.retriever.retrieve(\n",
    "            question,\n",
    "            top_k=top_k,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        if not contexts:\n",
    "            print(\"âš ï¸ No relevant contexts found\")\n",
    "            response = {\n",
    "                'answer': 'I could not find relevant information to answer this question.',\n",
    "                'sources': [],\n",
    "                'num_contexts_used': 0,\n",
    "                'question': question\n",
    "            }\n",
    "        else:\n",
    "            # Generate response\n",
    "            response = self.llm_client.generate_response(\n",
    "                query=question,\n",
    "                contexts=contexts,\n",
    "                **llm_kwargs\n",
    "            )\n",
    "            response['question'] = question\n",
    "            response['retrieved_contexts'] = contexts\n",
    "        \n",
    "        # Track conversation\n",
    "        self.conversation_history.append({\n",
    "            'question': question,\n",
    "            'answer': response.get('answer'),\n",
    "            'sources': response.get('sources', []),\n",
    "            'num_contexts': response.get('num_contexts_used', 0)\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def display_response(self, response: dict):\n",
    "        \"\"\"Display response in a formatted way\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âœ… ANSWER\")\n",
    "        print(\"=\"*70)\n",
    "        print(response.get('answer', 'No answer returned'))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"ğŸ“„ SOURCES ({response.get('num_contexts_used', 0)} documents)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        sources = response.get('sources', [])\n",
    "        if not sources:\n",
    "            print(\"  No sources available\")\n",
    "        else:\n",
    "            for src in sources:\n",
    "                if isinstance(src, dict):\n",
    "                    print(f\"  [{src.get('id', '?')}] {src.get('file', 'Unknown')} \"\n",
    "                          f\"(Page {src.get('page', 'N/A')}) - \"\n",
    "                          f\"Relevance: {src.get('similarity', 0.0):.1%}\")\n",
    "                else:\n",
    "                    print(f\"  - {src}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¤– Model: {response.get('model', 'N/A')} | \"\n",
    "              f\"Contexts: {response.get('num_contexts_used', 0)}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get session statistics\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return {\n",
    "                \"total_queries\": 0,\n",
    "                \"total_contexts_retrieved\": 0,\n",
    "                \"avg_contexts_per_query\": 0.0\n",
    "            }\n",
    "        \n",
    "        total_contexts = sum(c['num_contexts'] for c in self.conversation_history)\n",
    "        return {\n",
    "            \"total_queries\": len(self.conversation_history),\n",
    "            \"total_contexts_retrieved\": total_contexts,\n",
    "            \"avg_contexts_per_query\": total_contexts / len(self.conversation_history)\n",
    "        }\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"âœ… Conversation history cleared\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ INITIALIZING RAG SYSTEM\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 1: Load PDFs\n",
    "print(\"Step 1: Loading PDFs...\")\n",
    "all_pdf_documents = process_all_pdfs(\"data/pdf\")\n",
    "\n",
    "# Step 2: Split into chunks\n",
    "print(\"\\nStep 2: Splitting documents...\")\n",
    "chunks = split_documents(all_pdf_documents, chunk_size=800, chunk_overlap=100)\n",
    "\n",
    "# Step 3: Initialize Embedding Manager\n",
    "print(\"\\nStep 3: Initializing Embedding Manager...\")\n",
    "embedding_manager = EmbeddingManager()\n",
    "\n",
    "# Step 4: Generate embeddings\n",
    "print(\"\\nStep 4: Generating embeddings...\")\n",
    "texts = [chunk.page_content for chunk in chunks]  # âœ… Fixed: Extract text from chunks\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Step 5: Initialize Vector Store\n",
    "embedding_dim = embedding_manager.model.get_sentence_embedding_dimension()\n",
    "vectorstore = VectorStore(embedding_dim=embedding_dim)\n",
    "\n",
    "# Step 6: Add documents to vector store\n",
    "print(\"\\nStep 6: Adding documents to vector store...\")\n",
    "vectorstore.add_documents(chunks, embeddings)\n",
    "\n",
    "# Step 7: Initialize Groq Client\n",
    "print(\"\\nStep 7: Initializing Groq LLM Client...\")\n",
    "groq_client = GroqClient()  # Reads GROQ_API_KEY from environment\n",
    "\n",
    "# Step 8: Create RAG System\n",
    "print(\"\\nStep 8: Creating RAG System...\")\n",
    "rag_system = RAGSystem(\n",
    "    vector_store=vectorstore,\n",
    "    embedding_manager=embedding_manager,\n",
    "    llm_client=groq_client\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… SETUP COMPLETE - Ready to Query!\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nâ±ï¸ Total setup time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de01c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ¤– RUNNING TEST QUERIES\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "QUERY 1/3\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: What are the main topics of artificial intelligence covered in the documents?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'What are the main topics of artificial intelligenc...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a088b9b91291414e8fa9b181df97e7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 3 documents\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "The main topics of artificial intelligence covered in the documents include: \n",
      "a) General AI [1], \n",
      "b) Explainable AI (also referred to as \"Opening the Black Box\") [1], \n",
      "c) Advanced anonymisation protocols for data security and privacy [1], \n",
      "d) Machine Learning, which involves detecting patterns and making predictions by processing data and experiences [2], \n",
      "e) Weak AI vs. Strong AI, where Weak AI describes \"simulated\" thinking and Strong AI describes \"actual\" thinking [3]. \n",
      "\n",
      "Note: These topics are mentioned across different pages and sources within the provided context [1], [2], [3].\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (3 documents)\n",
      "======================================================================\n",
      "  [1] National-Strategy-for-Artificial-Intelligence.pdf (Page 61) - Relevance: 64.5%\n",
      "  [2] National-Strategy-for-Artificial-Intelligence.pdf (Page 95) - Relevance: 58.6%\n",
      "  [3] National-Strategy-for-Artificial-Intelligence.pdf (Page 14) - Relevance: 55.4%\n",
      "\n",
      "ğŸ¤– Model: llama-3.3-70b-versatile | Contexts: 3\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "QUERY 2/3\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: Explain the key findings from the research papers about neural networks.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'Explain the key findings from the research papers ...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669c3e5e96374869a9af6042692fa90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 3 documents\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "The research papers discuss the concept of neural networks, specifically Artificial Neural Networks (ANNs), which are algorithms based on the biological structure of the brain [1]. These networks have discrete layers and connections to other \"neurons\", with each layer picking out a specific feature to learn [1]. The papers also mention two major types of deep learning: Convolutional neural networks and Recurrent neural networks [3]. Convolutional neural networks are designed to extract complex features from unstructured data, such as images, while Recurrent neural networks can store information in context nodes, allowing them to learn data sequences and output a number or another sequence [3]. Additionally, Google's research on neural networks has led to developments in areas like natural language processing, speech translation, and search ranking and prediction systems [2]. \n",
      "\n",
      "Note: The context provided does not contain exhaustive information about the research papers, but it gives an overview of the key concepts and findings related to neural networks [1, 2, 3].\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (3 documents)\n",
      "======================================================================\n",
      "  [1] National-Strategy-for-Artificial-Intelligence.pdf (Page 13) - Relevance: 60.2%\n",
      "  [2] ML.pdf (Page 10) - Relevance: 58.3%\n",
      "  [3] National-Strategy-for-Artificial-Intelligence.pdf (Page 98) - Relevance: 57.6%\n",
      "\n",
      "ğŸ¤– Model: llama-3.3-70b-versatile | Contexts: 3\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "QUERY 3/3\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: What methodologies are discussed regarding agentic ai?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'What methodologies are discussed regarding agentic...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0fc291dc8d412ea53bad4bd373f257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 3 documents\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "The methodologies discussed regarding Agentic AI include Architectural Approaches, Learning Paradigms, Training Techniques, and Tools, as shown in Figure 3 [1]. Additionally, Agentic AI is characterized by goal-oriented, input-formed, and adaptable characteristics that allow it to accomplish intricate and multi-layered tasks [3]. \n",
      "\n",
      "Note: The context does not provide an exhaustive list of methodologies, but it highlights some key aspects of Agentic AI development [1], [2], [3].\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (3 documents)\n",
      "======================================================================\n",
      "  [1] Agentic_AI_Autonomous_Intelligence_for_Complex_GoalsA_Comprehensive_Survey.pdf (Page 8) - Relevance: 76.1%\n",
      "  [2] Agentic_AI_Autonomous_Intelligence_for_Complex_GoalsA_Comprehensive_Survey.pdf (Page 1) - Relevance: 74.5%\n",
      "  [3] Agentic_AI_Autonomous_Intelligence_for_Complex_GoalsA_Comprehensive_Survey.pdf (Page 1) - Relevance: 74.5%\n",
      "\n",
      "ğŸ¤– Model: llama-3.3-70b-versatile | Contexts: 3\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š SESSION STATISTICS\n",
      "======================================================================\n",
      "Total Queries: 3\n",
      "Total Contexts Retrieved: 9\n",
      "Avg Contexts per Query: 3.0\n",
      "======================================================================\n",
      "\n",
      "âœ… RAG System ready for interactive queries!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 9. EXAMPLE QUERIES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¤– RUNNING TEST QUERIES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What are the main topics of artificial intelligence covered in the documents?\",\n",
    "    \"Explain the key findings from the research papers about neural networks.\",\n",
    "    \"What methodologies are discussed regarding agentic ai?\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'â”'*70}\")\n",
    "    print(f\"QUERY {i}/{len(test_queries)}\")\n",
    "    print(f\"{'â”'*70}\")\n",
    "    \n",
    "    response = rag_system.query(\n",
    "        question=query,\n",
    "        top_k=3,\n",
    "        score_threshold=0.5,\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    rag_system.display_response(response)\n",
    "\n",
    "# ============================================================================\n",
    "# 10. SESSION STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š SESSION STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "stats = rag_system.get_stats()\n",
    "print(f\"Total Queries: {stats['total_queries']}\")\n",
    "print(f\"Total Contexts Retrieved: {stats['total_contexts_retrieved']}\")\n",
    "print(f\"Avg Contexts per Query: {stats['avg_contexts_per_query']:.1f}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"âœ… RAG System ready for interactive queries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944063cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¬ Interactive Mode - Ask questions about your documents!\n",
      "   (Type 'exit' to stop)\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: what is artificial intelligence?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'what is artificial intelligence?...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd4654d86fc48f59e4e00a9f14f987a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: how to implement RAG system\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'how to implement RAG system...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d84e6109a8247fc93088b9c0ac19f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: RAG\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'RAG...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b08c02b03594b8587c905d939faf53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: hello\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'hello...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0adee5a5ac459db6f73aeb6538fdfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: What is ML\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'What is ML...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af839b0a5d834fb5b79a8ad3363dd981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "ğŸ‘‹ Goodbye!\n",
      "Testing conversation memory...\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: \n",
      "\n",
      "Previous conversation:\n",
      "Q: hello\n",
      "A: I could not find relevant information to answer this question....\n",
      "Q: What is ML\n",
      "A: I could not find relevant information to answer this question....\n",
      "\n",
      "\n",
      "Current question: What is RAG?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: '\n",
      "\n",
      "Previous conversation:\n",
      "Q: hello\n",
      "A: I could not f...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815f349ea062492aa0c18c8d323f80a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: \n",
      "\n",
      "Previous conversation:\n",
      "Q: What is ML\n",
      "A: I could not find relevant information to answer this question....\n",
      "Q: \n",
      "\n",
      "Previous conversation:\n",
      "Q: hello\n",
      "A: I could not find relevant information to answer this question....\n",
      "Q: What is ML\n",
      "A: I could not find relevant information to answer this question....\n",
      "\n",
      "\n",
      "Current question: What is RAG?\n",
      "A: I could not find relevant information to answer this question....\n",
      "\n",
      "\n",
      "Current question: What are its benefits?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: '\n",
      "\n",
      "Previous conversation:\n",
      "Q: What is ML\n",
      "A: I could ...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2263b41f37034763830cf2e86cf54d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents (after filtering)\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INTERACTIVE QUERY MODE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ’¬ Interactive Mode - Ask questions about your documents!\")\n",
    "print(\"   (Type 'exit' to stop)\\n\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"Your question: \").strip()\n",
    "    \n",
    "    if user_query.lower() in ['exit', 'quit', 'q', '']:\n",
    "        print(\"\\nğŸ‘‹ Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    response = rag_system.query(\n",
    "        question=user_query,\n",
    "        top_k=3,\n",
    "        score_threshold=0.5,\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.3,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    \n",
    "    rag_system.display_response(response)\n",
    "\n",
    "    # Add this cell after your interactive mode\n",
    "\n",
    "def query_with_memory(rag_system, question: str, top_k: int = 3):\n",
    "    \"\"\"Query with awareness of previous conversation\"\"\"\n",
    "    # Get last 2 exchanges\n",
    "    history = rag_system.conversation_history[-2:] if rag_system.conversation_history else []\n",
    "    \n",
    "    history_context = \"\"\n",
    "    if history:\n",
    "        history_context = \"\\n\\nPrevious conversation:\\n\"\n",
    "        for conv in history:\n",
    "            history_context += f\"Q: {conv['question']}\\nA: {conv['answer'][:100]}...\\n\"\n",
    "    \n",
    "    enhanced_question = f\"{history_context}\\n\\nCurrent question: {question}\"\n",
    "    \n",
    "    response = rag_system.query(\n",
    "        question=enhanced_question,\n",
    "        top_k=top_k,\n",
    "        score_threshold=0.5\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Test multi-turn conversation\n",
    "print(\"Testing conversation memory...\\n\")\n",
    "q1 = query_with_memory(rag_system, \"What is RAG?\")\n",
    "rag_system.display_response(q1)\n",
    "\n",
    "q2 = query_with_memory(rag_system, \"What are its benefits?\")\n",
    "rag_system.display_response(q2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f718e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "import faiss\n",
    "from datetime import datetime\n",
    "\n",
    "class RAGSystemSaver:\n",
    "    \"\"\"Save/load FAISS-based RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir=\"rag_saves\"):\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    def save_all(self, rag_system, name=\"default\"):\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        base_path = os.path.join(self.save_dir, name)\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "        \n",
    "        # 1ï¸âƒ£ Save FAISS index\n",
    "        index_path = f\"{base_path}/faiss.index\"\n",
    "        faiss.write_index(rag_system.vector_store.index, index_path)\n",
    "        \n",
    "        # 2ï¸âƒ£ Save documents + metadata\n",
    "        docs_path = f\"{base_path}/documents.json\"\n",
    "        with open(docs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    \"documents\": rag_system.vector_store.documents,\n",
    "                    \"metadatas\": rag_system.vector_store.metadatas\n",
    "                },\n",
    "                f,\n",
    "                indent=2\n",
    "            )\n",
    "        \n",
    "        # 3ï¸âƒ£ Save config\n",
    "        config = {\n",
    "            \"name\": name,\n",
    "            \"created_at\": timestamp,\n",
    "            \"embedding_model\": rag_system.embedding_manager.model_name,\n",
    "            \"embedding_dim\": rag_system.vector_store.embedding_dim,\n",
    "            \"total_documents\": len(rag_system.vector_store.documents)\n",
    "        }\n",
    "        \n",
    "        with open(f\"{base_path}/config.json\", \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        # 4ï¸âƒ£ Save conversations to SQLite\n",
    "        db_path = f\"{base_path}/conversations.db\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS conversations (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                question TEXT,\n",
    "                answer TEXT,\n",
    "                num_contexts INTEGER,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        for conv in rag_system.conversation_history:\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO conversations (question, answer, num_contexts)\n",
    "                VALUES (?, ?, ?)\n",
    "            \"\"\", (\n",
    "                conv[\"question\"],\n",
    "                conv[\"answer\"],\n",
    "                conv[\"num_contexts\"]\n",
    "            ))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"âœ… RAG system saved successfully\")\n",
    "        print(f\"   Path: {base_path}\")\n",
    "        print(f\"   FAISS index: faiss.index\")\n",
    "        print(f\"   Documents: documents.json\")\n",
    "        print(f\"   Conversations: conversations.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57f984dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing query expansion...\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'Compare different RAG architectures...'\n",
      "   Top K: 2, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb159289964d40f7b9cb3c605a7c410f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 2 documents\n",
      "ğŸ” Retrieving documents for query: 'How do various RAG architectures differ from one a...'\n",
      "   Top K: 2, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7d3480ccde45d39ba1b7bf8b5be6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 2 documents\n",
      "ğŸ” Retrieving documents for query: 'What are the similarities and differences between ...'\n",
      "   Top K: 2, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de90384bd42443baba88d4b934bc03df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 2 documents\n",
      "Retrieved 3 unique contexts using query expansion\n"
     ]
    }
   ],
   "source": [
    "def expand_query(rag_system, question: str):\n",
    "    \"\"\"Generate alternative phrasings to improve retrieval\"\"\"\n",
    "    prompt = f\"\"\"Generate 2 alternative phrasings of this question:\n",
    "\"{question}\"\n",
    "\n",
    "Return ONLY the 2 questions, one per line. No numbering.\"\"\"\n",
    "    \n",
    "    response = rag_system.llm_client.client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    alternatives = response.choices[0].message.content.strip().split('\\n')\n",
    "    return [question] + alternatives[:2]\n",
    "\n",
    "def multi_query_retrieve(rag_system, question: str, top_k: int = 3):\n",
    "    \"\"\"Retrieve using multiple query variations\"\"\"\n",
    "    queries = expand_query(rag_system, question)\n",
    "    \n",
    "    all_contexts = []\n",
    "    seen_docs = set()\n",
    "    \n",
    "    for q in queries:\n",
    "        contexts = rag_system.retriever.retrieve(q, top_k=2, score_threshold=0.5)\n",
    "        for ctx in contexts:\n",
    "            doc_id = f\"{ctx['metadata'].get('source_file')}_{ctx['metadata'].get('page')}\"\n",
    "            if doc_id not in seen_docs:\n",
    "                all_contexts.append(ctx)\n",
    "                seen_docs.add(doc_id)\n",
    "    \n",
    "    return all_contexts[:top_k]\n",
    "\n",
    "# Test multi-query retrieval\n",
    "print(\"Testing query expansion...\\n\")\n",
    "expanded_question = \"Compare different RAG architectures\"\n",
    "better_contexts = multi_query_retrieve(rag_system, expanded_question)\n",
    "print(f\"Retrieved {len(better_contexts)} unique contexts using query expansion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd49b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: What are the main findings?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Retrieving documents for query: 'What are the main findings?...'\n",
      "   Top K: 3, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fea857a4cbb4aa4ad1e3d82a43462c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings with shape: (1, 384)\n",
      "âœ… Retrieved 0 documents\n",
      "âš ï¸ No relevant contexts found\n",
      "\n",
      "======================================================================\n",
      "âœ… ANSWER\n",
      "======================================================================\n",
      "I could not find relevant information to answer this question.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ SOURCES (0 documents)\n",
      "======================================================================\n",
      "  No sources available\n",
      "\n",
      "ğŸ¤– Model: N/A | Contexts: 0\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query_with_confidence_check(rag_system, question: str, top_k: int = 3):\n",
    "    \"\"\"Query with confidence threshold warnings\"\"\"\n",
    "    response = rag_system.query(question, top_k=top_k, score_threshold=0.5)\n",
    "    \n",
    "    if response.get('sources'):\n",
    "        avg_similarity = np.mean([src['similarity'] for src in response['sources']])\n",
    "        \n",
    "        if avg_similarity < 0.6:\n",
    "            print(f\"âš ï¸ WARNING: Low confidence score ({avg_similarity:.1%})\")\n",
    "            print(\"   Results may be unreliable. Try a different question.\\n\")\n",
    "        elif avg_similarity < 0.75:\n",
    "            print(f\"â„¹ï¸ MODERATE confidence ({avg_similarity:.1%})\")\n",
    "        else:\n",
    "            print(f\"âœ… HIGH confidence ({avg_similarity:.1%})\\n\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test\n",
    "response = query_with_confidence_check(rag_system, \"What are the main findings?\")\n",
    "rag_system.display_response(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
